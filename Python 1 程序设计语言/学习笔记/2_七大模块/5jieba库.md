# 模块5：jieba库

[jieba库文档](https://github.com/fxsjy/jieba)

## jieba库概述

jieba库是优秀的中文分词**第三方库**。（需要pip安装）



### 安装

cmd 命令行，执行：

```python
python -m pip install jieba
```



### 分词原理

jieba分词依靠中文词库，利用一个中文词库，确定中文字符之间的关联概率。

中文字符间概率大的组成词组，形成分词结果。除了分词，用户还可以添加自定义的词组。



## 三种模式

| 模式         | 说明                                       |
| ------------ | ------------------------------------------ |
| 全模式       | 把文本中所有可能的词语都扫描出来，有冗余。 |
| 精确模式     | 把文本精确的切分开，不存在冗余单词。       |
| 搜索引擎模式 | 在精确模式基础上，对长词再次切分。         |



## 常用函数

#### 精确模式

```python
#精确模式，返回一个列表类型的分词结果
#jieba.lcut(s)
```

示例：

```python
import jieba
jieba.lcut("中国是一个伟大的国家")

#['中国', '是', '一个', '伟大', '的', '国家']
```



#### 全模式

```python
#全模式，返回一个列表类型的分词结果，存在冗余
#jieba.lcut(s, cut_all = True)
```

示例：

```python
import jieba
jieba.lcut("中国是一个伟大的国家", cut_all = True)

#['中国', '国是', '一个', '伟大', '的', '国家']
```



#### 搜索引擎模式

```python
#搜索引擎模式，返回一个列表类型的分词结果，存在冗余
#jieba.lcut_for_search(s)
```

示例：

```python
import jieba
jieba.lcut_for_search("中华人民共和国是一个伟大的国家")

#['中华', '华人', '人民', '共和', '共和国', '中华人民共和国', '是', '一个', '伟大', '的', '国家']
```



#### 向分词词典中增加新词

```python
#jieba.add_word(w)
```





